{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d842b448",
   "metadata": {},
   "source": [
    "# Pre-generate some probabalistic scaling factors for ERF\n",
    "\n",
    "Based on AR6 Chapter 7 ERF uncertainty\n",
    "\n",
    "We do not modify forcing scale factors for ozone and aerosols, because we adjust the precursor species to span the forcing uncertainty this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef524d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "from fair import __version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfc0316",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_v = dotenv_values('../../.env')['CALIBRATION_VERSION']\n",
    "fair_v = dotenv_values('../../.env')['FAIR_VERSION']\n",
    "samples = int(dotenv_values('../../.env')['PRIOR_SAMPLES'])\n",
    "assert fair_v == __version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54299745",
   "metadata": {},
   "outputs": [],
   "source": [
    "NINETY_TO_ONESIGMA = scipy.stats.norm.ppf(0.95)\n",
    "NINETY_TO_ONESIGMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a958cb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_u90 = {\n",
    "#    'CO2': 0.12,      # CO2\n",
    "    'CH4': 0.20,      # CH4: updated value from etminan 2016\n",
    "    'N2O': 0.14,      # N2O\n",
    "    'minorGHG': 0.19,      # other WMGHGs\n",
    "    'Stratospheric water vapour': 1.00,\n",
    "    'Contrails' : 0.70,      # contrails approx - half-normal\n",
    "    'Light absorbing particles on snow and ice': 1.25,      # bc on snow - half-normal\n",
    "    'Land use': 0.50,      # land use change\n",
    "    'Volcanic': 5.0/20.0,  # needs to be way bigger?\n",
    "    'solar_amplitude': 0.50,\n",
    "    'solar_trend': 0.07,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb93ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "seedgen = 380133900\n",
    "scalings = {}\n",
    "for forcer in forcing_u90:\n",
    "    scalings[forcer] = scipy.stats.norm.rvs(1, forcing_u90[forcer]/NINETY_TO_ONESIGMA, size=samples, random_state=seedgen)\n",
    "    seedgen=seedgen+112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b85ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalings['CH4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0300dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LAPSI is asymmetric Gaussian. We can just scale the half of the distribution above/below best estimate\n",
    "scalings['Light absorbing particles on snow and ice'][scalings['Light absorbing particles on snow and ice']<1] = 0.08/0.1*(scalings['Light absorbing particles on snow and ice'][scalings['Light absorbing particles on snow and ice']<1]-1) + 1\n",
    "\n",
    "## so is contrails - the benefits of doing this are tiny :)\n",
    "scalings['Contrails'][scalings['Contrails']<1] = 0.0384/0.0406*(scalings['Contrails'][scalings['Contrails']<1]-1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d818d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solar trend is absolute, not scaled\n",
    "scalings['solar_trend'] = scalings['solar_trend'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.hist(scalings['Light absorbing particles on snow and ice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b7c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take CO2 scaling from 4xCO2 generated from the EBMs\n",
    "df_ebm = pd.read_csv(f'../../output/fair-{fair_v}/v{cal_v}/priors/climate_response_ebm3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f57a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalings['CO2'] = np.array(1 + 0.563*(df_ebm['F_4xCO2'].mean() - df_ebm['F_4xCO2'])/df_ebm['F_4xCO2'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d91bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803bc4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.DataFrame(scalings, columns=scalings.keys())\n",
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc2e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.quantile((.05, 0.50, .95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad17acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv(f'../../output/fair-{fair_v}/v{cal_v}/priors/forcing_scaling.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ec4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
